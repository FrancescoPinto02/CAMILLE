filename,function_name,smell,name_smell,message
C:\Users\Francesco\PycharmProjects\CAMILLE\cs_detection_tool\input\tests\UKPLab\sentence-transformers\examples\training\distillation\model_distillation.py,deduplicate,1,columns_and_datatype_not_explicitly_set,columns and datatype not explicit
C:\Users\Francesco\PycharmProjects\CAMILLE\cs_detection_tool\input\tests\UKPLab\sentence-transformers\examples\training\distillation\model_distillation_layer_reduction.py,deduplicate,1,columns_and_datatype_not_explicitly_set,columns and datatype not explicit
C:\Users\Francesco\PycharmProjects\CAMILLE\cs_detection_tool\input\tests\UKPLab\sentence-transformers\sentence_transformers\SentenceTransformer.py,encode,1,pytorch_call_method_misused,is recommended to use self.net()
C:\Users\Francesco\PycharmProjects\CAMILLE\cs_detection_tool\input\tests\UKPLab\sentence-transformers\sentence_transformers\losses\CachedGISTEmbedLoss.py,_backward_hook,2,gradients_not_cleared_before_backward_propagation,Please consider to use zero_grad() before backward().
C:\Users\Francesco\PycharmProjects\CAMILLE\cs_detection_tool\input\tests\UKPLab\sentence-transformers\sentence_transformers\losses\CachedGISTEmbedLoss.py,calculate_loss_and_cache_gradients,1,gradients_not_cleared_before_backward_propagation,Please consider to use zero_grad() before backward().
C:\Users\Francesco\PycharmProjects\CAMILLE\cs_detection_tool\input\tests\UKPLab\sentence-transformers\sentence_transformers\losses\CachedMultipleNegativesRankingLoss.py,_backward_hook,2,gradients_not_cleared_before_backward_propagation,Please consider to use zero_grad() before backward().
C:\Users\Francesco\PycharmProjects\CAMILLE\cs_detection_tool\input\tests\UKPLab\sentence-transformers\sentence_transformers\losses\CachedMultipleNegativesRankingLoss.py,calculate_loss_and_cache_gradients,1,gradients_not_cleared_before_backward_propagation,Please consider to use zero_grad() before backward().
C:\Users\Francesco\PycharmProjects\CAMILLE\cs_detection_tool\input\tests\UKPLab\sentence-transformers\sentence_transformers\losses\MegaBatchMarginLoss.py,forward_mini_batched,1,gradients_not_cleared_before_backward_propagation,Please consider to use zero_grad() before backward().
